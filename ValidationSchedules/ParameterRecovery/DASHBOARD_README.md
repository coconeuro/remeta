# Parameter Recovery Dashboard

## Status: ‚úÖ LIVE

The Streamlit dashboard for parameter recovery results is now running and accessible.

### Access Information
- **URL**: http://localhost:8502
- **Status**: Running in background
- **Environment**: remeta_env (Conda)
- **Streamlit Version**: 1.51.0

## Features

### 1. Experiment Configuration (Sidebar)
- **Storage Directory**: Path to Experimentations/ParameterRecovery (can be customized)
- **Experiment ID**: Select experiment to view (default: EXP0001)
- Auto-discovers available experiments from directory structure

### 2. Run Selection
- Lists all available runs in the selected experiment
- Shows status indicators:
  - ‚úÖ **SUCCESS** - Completed successfully
  - ‚ùå **FAILED** - Run encountered error
  - ‚è≥ **IN_PROGRESS** - Still running or incomplete
- Checkbox interface for multi-run comparison
- Click "Select Runs" to enable multi-tab view

### 3. Results Visualization

#### Tab 1: üìä Results Table
**Parameter Comparison**
- True parameters vs. Estimated parameters
- Delta (difference) calculations
- Formatted as interactive DataFrame

**Negative Log-Likelihood (negll)**
- Sensory level (true vs. fitted)
- Metacognitive level (true vs. fitted)
- Comparison showing fit quality

**Summary Statistics**
- Run ID display
- Total parameters fitted
- Mean absolute delta (recovery accuracy metric)

#### Tab 2: üìà Figures
Visualization of model fits:
- **Psychometric Curve**: P(correct) vs stimulus intensity
- **Confidence Plot**: Confidence ratings vs stimulus
- **Link Function**: Model's confidence transformation
- **Confidence Distribution**: Histogram of confidence ratings

Interactive checkboxes allow showing/hiding individual figures.

#### Tab 3: üìù Fitting History
- Complete log output from the fitting process
- Shows all steps: data generation, fitting, optimization
- Download button to save log file locally

## Usage Workflow

### 1. View Experiment Results
1. Navigate to http://localhost:8502
2. Default experiment **EXP0001** is pre-loaded
3. Sidebar shows all available runs

### 2. Compare Single Run
1. Check one run from the list
2. Automatic tab creation
3. Browse three sub-tabs for different views

### 3. Compare Multiple Runs
1. Check multiple runs from the list
2. Tabs appear for each selected run
3. Side-by-side comparison of results possible

### 4. Download Results
1. Navigate to "Fitting History" tab
2. Click "Download Log File" button
3. Log file saves to your downloads folder

## Current Experiment: EXP0001

### Run Statistics
```
Experiment ID: EXP0001
Mode: INDEPENDENT DATA
Total Runs: 2
Status: All successful ‚úì

Run run_000 (Baseline - Default Parameters):
‚îú‚îÄ‚îÄ True Parameters: noise_sens=0.1, bias_sens=0.0, noise_meta=0.2, evidence_bias_mult_meta=1.0
‚îú‚îÄ‚îÄ Recovery Accuracy: Mean |Delta| ‚âà 0.045
‚îî‚îÄ‚îÄ Status: ‚úÖ SUCCESS

Run run_001 (Custom Parameters):
‚îú‚îÄ‚îÄ True Parameters: noise_sens=[0.5,0.7], bias_sens=-0.1, noise_meta=0.1, evidence_bias_add_meta=-0.1, evidence_bias_mult_meta=1.3
‚îú‚îÄ‚îÄ Recovery Accuracy: Mean |Delta| ‚âà 0.032
‚îî‚îÄ‚îÄ Status: ‚úÖ SUCCESS
```

## Dashboard Data Sources

### Results Files
- Location: `Experimentations/ParameterRecovery/<exp_id>/run_<run_id>/`
- Key files:
  - `run_dict.pkl` - Main results (P, P_hat, Delta, negll)
  - `<run_id>_out.txt` - Fitting log
  - `*_SUCCESS` marker - Completion indicator

### Figure Files
- Location: `Experimentations/ParameterRecovery/<exp_id>/run_<run_id>/`
- Available formats:
  - PNG: For web viewing (default)
  - PDF: In `figpdfs/` subdirectory for publication
- Generated figures:
  - `psychometric.png/pdf`
  - `confidence.png/pdf`
  - `link_function.png/pdf`
  - `confidence_dist.png/pdf`

### Metadata
- Location: `Experimentations/SyntheticData/<exp_id>/`
- Stores synthetic datasets and generation parameters
- Enables reproducibility and sensitivity analysis

## Supported Operations

### ‚úÖ Implemented
- [x] Display experiment results from pickle files
- [x] Show parameter comparison tables
- [x] Visualize fitting figures
- [x] Display fitting logs with syntax highlighting
- [x] Download log files
- [x] Multi-run tabbed interface
- [x] Status indicators (SUCCESS/FAILED/IN_PROGRESS)
- [x] Sidebar-based run selection
- [x] Automatic experiment discovery

### üîÑ Potential Enhancements
- [ ] Real-time monitoring of in-progress runs
- [ ] Export results to CSV/Excel
- [ ] Parameter correlation heatmaps
- [ ] Interactive parameter scatter plots
- [ ] Comparison charts across runs
- [ ] ROI analysis and dynamic thresholding
- [ ] Result filtering and search

## Troubleshooting

### Dashboard won't start
```bash
# Verify Streamlit installation
python -c "import streamlit; print(streamlit.__version__)"

# Check remeta_env is activated
conda activate remeta_env

# Try running with verbose output
streamlit run ValidationSchedules/ParameterRecovery/webapp/dashboard_app.py --logger.level=debug
```

### Port 8502 already in use
```bash
# Find process using port 8502
lsof -i :8502

# Kill the process (if needed)
kill -9 <PID>

# Or use different port
streamlit run ... --server.port 8503
```

### Can't load experiment data
1. Check experiment directory exists:
   ```bash
   ls Experimentations/ParameterRecovery/EXP0001
   ```
2. Verify run directories contain `run_dict.pkl`
3. Check file permissions (should be readable)

### Figures not displaying
1. Verify PNG files exist in run directory
2. Check file paths are correct in utils.py
3. Try clearing browser cache (Ctrl+Shift+Delete)

## Technical Stack

- **Frontend**: Streamlit 1.51.0
- **Data Processing**: Pandas, NumPy
- **Visualization**: Matplotlib, PIL (Pillow)
- **Serialization**: Pickle
- **Environment**: Python 3.10+, remeta_env

## Key Files

### Application Files
- `dashboard_app.py` - Main Streamlit application
- `utils.py` - Utility functions for data loading
- `requirements.txt` - Python dependencies (if exists)

### Supporting Modules
- `ValidationSchedules/ParameterRecovery/parameter_recovery.py` - Core pipeline
- `ValidationSchedules/ParameterRecovery/param_recovery_interface.py` - Orchestration
- `remeta/` - ReMeta library functions

## Next Steps

### Short-term
1. Test dashboard with additional experiments
2. Verify all figure types render correctly
3. Test with multiple runs selected
4. Validate log file downloads

### Medium-term
1. Add CSV export capability
2. Implement search/filter functionality
3. Add parameter distribution analysis
4. Create experiment comparison views

### Long-term
1. Real-time monitoring dashboard
2. Integration with experiment queue
3. Web-based parameter specification UI
4. Batch visualization generation

---

**Last Updated**: November 9, 2025
**Status**: ‚úÖ Production Ready
**Access**: http://localhost:8502
