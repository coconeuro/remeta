{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-21T21:04:52.251346799Z",
     "start_time": "2026-01-21T21:04:52.165164864Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "Go directly to:\n",
    "- [**Start page**](https://github.com/coconeuro/remeta/)\n",
    "- [**Installation**](https://github.com/coconeuro/remeta/blob/main/INSTALL.md)\n",
    "- [**Basic Usage**](https://github.com/coconeuro/remeta/blob/main/demo/basic_usage.ipynb)\n",
    "- [**Common use cases**](https://github.com/coconeuro/remeta/blob/main/demo/common_use_cases.ipynb)\n",
    "- [**Group estimation and priors** (this page)](https://github.com/coconeuro/remeta/blob/main/demo/group_estimation_priors.ipynb)"
   ],
   "id": "26fa195842dd5432"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T21:05:24.572878734Z",
     "start_time": "2026-01-21T21:05:24.523017248Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Group estimation and priors\n",
    "\n",
    "Often, data from single participants is not sufficient for precise parameter estimation. In this case, ReMeta offers three different options to enrich parameter estimation either by group-level or prior information."
   ],
   "id": "93c8a346bcec4834"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Group estimation (fixed effect)\n",
    "\n",
    "To use group-level information we need to pass 1) a 2d data to ReMeta (nsubjects x nsamples) and 2) specify the `group` attribute for parameters.\n",
    "\n",
    "The `fit` method of ReMeta accepts data as either 1d arrays (single participant) or as 2d arrays (group data). To showcase, we simulate type 1 data for 4 participants:\n"
   ],
   "id": "d8a6cc218325d291"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T20:03:53.470336333Z",
     "start_time": "2026-02-17T20:03:53.432670517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import remeta\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "cfg = remeta.Configuration()\n",
    "cfg.param_type1_nonlinear_gain.enable = 1\n",
    "cfg.skip_type2 = True\n",
    "params_true = dict(\n",
    "    type1_noise=0.5,\n",
    "    type1_bias=0,\n",
    "    type1_nonlinear_gain=0.5\n",
    ")\n",
    "cfg.true_params = params_true\n",
    "data = remeta.simulate(nsubjects=4, nsamples=2000, params=params_true, cfg=cfg)"
   ],
   "id": "589266156b19108",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "..Generative parameters:\n",
      "    Type 1 noise distribution: normal\n",
      "    type1_noise: 0.5\n",
      "    type1_bias: 0\n",
      "    type1_nonlinear_gain: 0.5\n",
      "..Descriptive statistics:\n",
      "    No. subjects: 4\n",
      "    No. samples: 2000\n",
      "    Accuracy: 84.4% correct\n",
      "    d': 2.0\n",
      "    Choice bias: -0.4%\n",
      "----------------------------------\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We simulated data with a slight non-linearity (`type1_param_nonlinear_gain=0.5`). Non-linearities in the encoding of the stimulus intensity are notoriously sample-intensive and thus estimates for single participants are often all over the place even if fitted with the same ground truth model. This is exactly what we find, even though each participant has 2000 trials:",
   "id": "71f9f90a196bc97d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T20:03:54.578849032Z",
     "start_time": "2026-02-17T20:03:53.472524215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rem = remeta.ReMeta(cfg=cfg)\n",
    "rem.fit(data.stimuli, data.choices, data.confidence)\n",
    "result = rem.summary()"
   ],
   "id": "642519c42d6b867b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset characteristics:\n",
      "    No. subjects: 4\n",
      "    No. samples: [2000, 2000, 2000, 2000]\n",
      "    Accuracy: 84.4% correct\n",
      "    d': 2.023\n",
      "    Choice bias: -0.4%\n",
      "\n",
      "+++ Type 1 level +++\n",
      "  Subject-level estimation (MLE)\n",
      "     Subject 1 / 4\n",
      "     Subject 2 / 4\n",
      "     Subject 3 / 4\n",
      "     Subject 4 / 4\n",
      "    .. finished (1.1 secs).\n",
      "  Final report\n",
      "    Subject 1 / 4\n",
      "        Parameters estimates (subject-level fit)\n",
      "            [subject] type1_noise: 0.536 ± 0.056 (true: 0.500)\n",
      "            [subject] type1_nonlinear_gain: 0.505 ± 0.373 (true: 0.500)\n",
      "            [subject] type1_bias: -0.023 ± 0.020 (true: 0.000)\n",
      "        [subject] Log-likelihood: -748.56 (per sample: -0.3743)\n",
      "        [subject] Fitting time: 0.23 secs\n",
      "        Log-likelihood using true params: -1177.70 (per sample: -0.5888)\n",
      "    Subject 2 / 4\n",
      "        Parameters estimates (subject-level fit)\n",
      "            [subject] type1_noise: 0.472 ± 0.039 (true: 0.500)\n",
      "            [subject] type1_nonlinear_gain: 0.107 ± 0.264 (true: 0.500)\n",
      "            [subject] type1_bias: -0.026 ± 0.018 (true: 0.000)\n",
      "        [subject] Log-likelihood: -735.23 (per sample: -0.3676)\n",
      "        [subject] Fitting time: 0.19 secs\n",
      "        Log-likelihood using true params: -1169.71 (per sample: -0.5849)\n",
      "    Subject 3 / 4\n",
      "        Parameters estimates (subject-level fit)\n",
      "            [subject] type1_noise: 0.482 ± 0.047 (true: 0.500)\n",
      "            [subject] type1_nonlinear_gain: 0.569 ± 0.421 (true: 0.500)\n",
      "            [subject] type1_bias: 0.007 ± 0.019 (true: 0.000)\n",
      "        [subject] Log-likelihood: -673.61 (per sample: -0.3368)\n",
      "        [subject] Fitting time: 0.29 secs\n",
      "        Log-likelihood using true params: -1060.33 (per sample: -0.5302)\n",
      "    Subject 4 / 4\n",
      "        Parameters estimates (subject-level fit)\n",
      "            [subject] type1_noise: 0.508 ± 0.049 (true: 0.500)\n",
      "            [subject] type1_nonlinear_gain: 0.386 ± 0.339 (true: 0.500)\n",
      "            [subject] type1_bias: 0.005 ± 0.019 (true: 0.000)\n",
      "        [subject] Log-likelihood: -733.73 (per sample: -0.3669)\n",
      "        [subject] Fitting time: 0.12 secs\n",
      "        Log-likelihood using true params: -1119.98 (per sample: -0.56)\n",
      "Type 1 level finished\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In case of a group-level fit, the result returned by the `summary()` method is a list of length `nsubjects`. We can print the final parameter estimates more cleanly as follows:",
   "id": "cd2b50a3b6e63401"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T20:03:54.680294784Z",
     "start_time": "2026-02-17T20:03:54.633345354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for s in range(result.nsubjects):\n",
    "    print(f'Subject {s}')\n",
    "    for k, v in result.type1.params[s].items():\n",
    "        print(f'\\t{k}: {v:.3f}')"
   ],
   "id": "d5840a6d904b6689",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0\n",
      "\ttype1_noise: 0.536\n",
      "\ttype1_nonlinear_gain: 0.505\n",
      "\ttype1_bias: -0.023\n",
      "Subject 1\n",
      "\ttype1_noise: 0.472\n",
      "\ttype1_nonlinear_gain: 0.107\n",
      "\ttype1_bias: -0.026\n",
      "Subject 2\n",
      "\ttype1_noise: 0.482\n",
      "\ttype1_nonlinear_gain: 0.569\n",
      "\ttype1_bias: 0.007\n",
      "Subject 3\n",
      "\ttype1_noise: 0.508\n",
      "\ttype1_nonlinear_gain: 0.386\n",
      "\ttype1_bias: 0.005\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The fitted parameters for `type1_nonlinear_gain` vary strongly around the true value of `0.5`. Yet, this is no fitting error (the empirical negative log-likelihood is always lower than one for the true parameters), but caused by the fact that non-linearities are very hard to infer from binary choice data. Not impossible, but it takes lots of samples to get anywhere near an acceptable level f precision.\n",
    "\n",
    "One option to tackle this is to fit a single non-linearity parameter to the entire group. To specify, we set the `group` attribute of the parameter to `'fixed'`:"
   ],
   "id": "8f33efc7ad54b4e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T20:03:54.747083219Z",
     "start_time": "2026-02-17T20:03:54.694511616Z"
    }
   },
   "cell_type": "code",
   "source": "cfg.param_type1_nonlinear_gain.group = 'fixed'",
   "id": "1c45364c2c726908",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We fit the model as usual:",
   "id": "4f481216f0d54be9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T20:03:57.246110565Z",
     "start_time": "2026-02-17T20:03:54.750419388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rem = remeta.ReMeta(cfg=cfg)\n",
    "rem.fit(data.stimuli, data.choices, data.confidence)\n",
    "result = rem.summary()"
   ],
   "id": "25591c7a491e2fea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset characteristics:\n",
      "    No. subjects: 4\n",
      "    No. samples: [2000, 2000, 2000, 2000]\n",
      "    Accuracy: 84.4% correct\n",
      "    d': 2.023\n",
      "    Choice bias: -0.4%\n",
      "\n",
      "+++ Type 1 level +++\n",
      "  Subject-level estimation (MLE)\n",
      "     Subject 1 / 4\n",
      "     Subject 2 / 4\n",
      "     Subject 3 / 4\n",
      "     Subject 4 / 4\n",
      "    .. finished (1.1 secs).\n",
      "\n",
      "  Group-level optimization (MLE / MAP)\n",
      "        [21:03:55] Iteration 1 / 30\n",
      "        [21:03:56] Iteration 11 / 30\n",
      "        [21:03:56] Iteration 21 / 30\n",
      "    .. finished (1.3 secs).\n",
      "  Final report\n",
      "    Subject 1 / 4\n",
      "        Parameters estimates (subject-level fit)\n",
      "            [subject] type1_noise: 0.536 ± 0.056 (true: 0.500)\n",
      "            [subject] type1_nonlinear_gain: 0.505 ± 0.373 (true: 0.500)\n",
      "            [subject] type1_bias: -0.023 ± 0.020 (true: 0.000)\n",
      "        [subject] Log-likelihood: -748.56 (per sample: -0.3743)\n",
      "        [subject] Fitting time: 0.23 secs\n",
      "        Parameters estimates (group-level fit)\n",
      "            [subject] type1_noise: 0.518 ± 0.019 (true: 0.500)\n",
      "            [group=fixed] type1_nonlinear_gain: 0.381 ± 0.064 (true: 0.500)\n",
      "            [subject] type1_bias: -0.023 ± 0.019 (true: 0.000)\n",
      "        [final] Log-likelihood: -748.62 (per sample: -0.3743)\n",
      "        Log-likelihood using true params: -1177.70 (per sample: -0.5888)\n",
      "    Subject 2 / 4\n",
      "        Parameters estimates (subject-level fit)\n",
      "            [subject] type1_noise: 0.472 ± 0.039 (true: 0.500)\n",
      "            [subject] type1_nonlinear_gain: 0.107 ± 0.264 (true: 0.500)\n",
      "            [subject] type1_bias: -0.026 ± 0.018 (true: 0.000)\n",
      "        [subject] Log-likelihood: -735.23 (per sample: -0.3676)\n",
      "        [subject] Fitting time: 0.19 secs\n",
      "        Parameters estimates (group-level fit)\n",
      "            [subject] type1_noise: 0.509 ± 0.019 (true: 0.500)\n",
      "            [group=fixed] type1_nonlinear_gain: 0.381 ± 0.064 (true: 0.500)\n",
      "            [subject] type1_bias: -0.028 ± 0.019 (true: 0.000)\n",
      "        [final] Log-likelihood: -735.60 (per sample: -0.3678)\n",
      "        Log-likelihood using true params: -1169.71 (per sample: -0.5849)\n",
      "    Subject 3 / 4\n",
      "        Parameters estimates (subject-level fit)\n",
      "            [subject] type1_noise: 0.482 ± 0.047 (true: 0.500)\n",
      "            [subject] type1_nonlinear_gain: 0.569 ± 0.421 (true: 0.500)\n",
      "            [subject] type1_bias: 0.007 ± 0.019 (true: 0.000)\n",
      "        [subject] Log-likelihood: -673.61 (per sample: -0.3368)\n",
      "        [subject] Fitting time: 0.31 secs\n",
      "        Parameters estimates (group-level fit)\n",
      "            [subject] type1_noise: 0.460 ± 0.017 (true: 0.500)\n",
      "            [group=fixed] type1_nonlinear_gain: 0.381 ± 0.064 (true: 0.500)\n",
      "            [subject] type1_bias: 0.007 ± 0.018 (true: 0.000)\n",
      "        [final] Log-likelihood: -673.74 (per sample: -0.3369)\n",
      "        Log-likelihood using true params: -1060.33 (per sample: -0.5302)\n",
      "    Subject 4 / 4\n",
      "        Parameters estimates (subject-level fit)\n",
      "            [subject] type1_noise: 0.508 ± 0.049 (true: 0.500)\n",
      "            [subject] type1_nonlinear_gain: 0.386 ± 0.339 (true: 0.500)\n",
      "            [subject] type1_bias: 0.005 ± 0.019 (true: 0.000)\n",
      "        [subject] Log-likelihood: -733.73 (per sample: -0.3669)\n",
      "        [subject] Fitting time: 0.12 secs\n",
      "        Parameters estimates (group-level fit)\n",
      "            [subject] type1_noise: 0.507 ± 0.018 (true: 0.500)\n",
      "            [group=fixed] type1_nonlinear_gain: 0.381 ± 0.064 (true: 0.500)\n",
      "            [subject] type1_bias: 0.005 ± 0.019 (true: 0.000)\n",
      "        [final] Log-likelihood: -733.73 (per sample: -0.3669)\n",
      "        Log-likelihood using true params: -1119.98 (per sample: -0.56)\n",
      "Type 1 level finished\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, the parameter `type1_nonlinear_gain` is fitted to the entire group dataset and thus the parameter is identical for each participant. The final estimate of `0.492` much closer to the ground truth value of `0.5`. We once again print the final parameter more cleanly:",
   "id": "140d658f62c15d13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T20:03:57.401090851Z",
     "start_time": "2026-02-17T20:03:57.311534284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for s in range(result.nsubjects):\n",
    "    print(f'Subject {s}')\n",
    "    for k, v in result.type1.params[s].items():\n",
    "        print(f'\\t{k}: {v:.3f}')"
   ],
   "id": "44887a82ff58e9a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0\n",
      "\ttype1_noise: 0.518\n",
      "\ttype1_nonlinear_gain: 0.381\n",
      "\ttype1_bias: -0.023\n",
      "Subject 1\n",
      "\ttype1_noise: 0.509\n",
      "\ttype1_nonlinear_gain: 0.381\n",
      "\ttype1_bias: -0.028\n",
      "Subject 2\n",
      "\ttype1_noise: 0.460\n",
      "\ttype1_nonlinear_gain: 0.381\n",
      "\ttype1_bias: 0.007\n",
      "Subject 3\n",
      "\ttype1_noise: 0.507\n",
      "\ttype1_nonlinear_gain: 0.381\n",
      "\ttype1_bias: 0.005\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note that even though parameter recovery improved, the log-likelihood of this group fit is worse (i.e. lower) than the single-subject fit:",
   "id": "224a4be8f710b40c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T20:03:57.435412836Z",
     "start_time": "2026-02-17T20:03:57.402233883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for s in range(result.nsubjects):\n",
    "    print(f'Subject {s}')\n",
    "    print(f'\\tnegll(subject fit): {result.type1.subject.loglik[s]:.3f}')\n",
    "    print(f'\\tnegll(group fit): {result.type1.group.loglik[s]:.3f}')"
   ],
   "id": "d23aa330866373c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0\n",
      "\tnegll(subject fit): -748.555\n",
      "\tnegll(group fit): -748.615\n",
      "Subject 1\n",
      "\tnegll(subject fit): -735.225\n",
      "\tnegll(group fit): -735.602\n",
      "Subject 2\n",
      "\tnegll(subject fit): -673.607\n",
      "\tnegll(group fit): -673.740\n",
      "Subject 3\n",
      "\tnegll(subject fit): -733.734\n",
      "\tnegll(group fit): -733.734\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T15:26:46.374615841Z",
     "start_time": "2026-01-27T15:26:46.276004093Z"
    }
   },
   "cell_type": "markdown",
   "source": "Yet, in this case this effectively means that the model is not overfit to random peculiarities of each subject's data and better fits the broad trends in the group data.",
   "id": "922c2e9f164ca93e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Group estimation (random effect)\n",
    "\n",
    "Another possibility is to treat a parameter as a random effect. When modeled as a random effect, each subject is fitted to its own data, but the parameter estimate is informed / regularized by an estimate of the parameter's group distribution. If you assume that there is plausibly variability between participants (and there typically is), modeling parameters as random effects might be the better choice."
   ],
   "id": "25e0c41372c5f76e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T20:04:00.209686411Z",
     "start_time": "2026-02-17T20:03:57.439259531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cfg.param_type1_nonlinear_gain.group = 'random'\n",
    "rem = remeta.ReMeta(cfg=cfg)\n",
    "rem.fit(data.stimuli, data.choices, data.confidence)\n",
    "result = rem.summary()"
   ],
   "id": "e780cbdb8d60c24e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset characteristics:\n",
      "    No. subjects: 4\n",
      "    No. samples: [2000, 2000, 2000, 2000]\n",
      "    Accuracy: 84.4% correct\n",
      "    d': 2.023\n",
      "    Choice bias: -0.4%\n",
      "\n",
      "+++ Type 1 level +++\n",
      "  Subject-level estimation (MLE)\n",
      "     Subject 1 / 4\n",
      "     Subject 2 / 4\n",
      "     Subject 3 / 4\n",
      "     Subject 4 / 4\n",
      "    .. finished (1.1 secs).\n",
      "\n",
      "  Group-level optimization (MLE / MAP)\n",
      "        [21:03:58] Iteration 1 / 30 (Convergence: 0.00012781)\n",
      "        [21:03:59] Iteration 11 / 30 (Convergence: 0.00001832)\n",
      "        [21:03:59] Iteration 21 / 30 (Convergence: 0.00000382)\n",
      "    .. finished (1.6 secs).\n",
      "  Final report\n",
      "    Subject 1 / 4\n",
      "        Parameters estimates (subject-level fit)\n",
      "            [subject] type1_noise: 0.536 ± 0.056 (true: 0.500)\n",
      "            [subject] type1_nonlinear_gain: 0.505 ± 0.373 (true: 0.500)\n",
      "            [subject] type1_bias: -0.023 ± 0.020 (true: 0.000)\n",
      "        [subject] Log-likelihood: -748.56 (per sample: -0.3743)\n",
      "        [subject] Fitting time: 0.22 secs\n",
      "        Parameters estimates (group-level fit)\n",
      "            [subject] type1_noise: 0.520 ± 0.023 (true: 0.500)\n",
      "            [group=random] type1_nonlinear_gain: 0.389 ± 0.091 (true: 0.500)\n",
      "            [subject] type1_bias: -0.023 ± 0.019 (true: 0.000)\n",
      "        [final] Log-likelihood: -748.61 (per sample: -0.3743)\n",
      "        Log-likelihood using true params: -1177.70 (per sample: -0.5888)\n",
      "    Subject 2 / 4\n",
      "        Parameters estimates (subject-level fit)\n",
      "            [subject] type1_noise: 0.472 ± 0.039 (true: 0.500)\n",
      "            [subject] type1_nonlinear_gain: 0.107 ± 0.264 (true: 0.500)\n",
      "            [subject] type1_bias: -0.026 ± 0.018 (true: 0.000)\n",
      "        [subject] Log-likelihood: -735.23 (per sample: -0.3676)\n",
      "        [subject] Fitting time: 0.18 secs\n",
      "        Parameters estimates (group-level fit)\n",
      "            [subject] type1_noise: 0.507 ± 0.022 (true: 0.500)\n",
      "            [group=random] type1_nonlinear_gain: 0.361 ± 0.089 (true: 0.500)\n",
      "            [subject] type1_bias: -0.028 ± 0.019 (true: 0.000)\n",
      "        [final] Log-likelihood: -735.55 (per sample: -0.3678)\n",
      "        Log-likelihood using true params: -1169.71 (per sample: -0.5849)\n",
      "    Subject 3 / 4\n",
      "        Parameters estimates (subject-level fit)\n",
      "            [subject] type1_noise: 0.482 ± 0.047 (true: 0.500)\n",
      "            [subject] type1_nonlinear_gain: 0.569 ± 0.421 (true: 0.500)\n",
      "            [subject] type1_bias: 0.007 ± 0.019 (true: 0.000)\n",
      "        [subject] Log-likelihood: -673.61 (per sample: -0.3368)\n",
      "        [subject] Fitting time: 0.33 secs\n",
      "        Parameters estimates (group-level fit)\n",
      "            [subject] type1_noise: 0.462 ± 0.020 (true: 0.500)\n",
      "            [group=random] type1_nonlinear_gain: 0.393 ± 0.091 (true: 0.500)\n",
      "            [subject] type1_bias: 0.007 ± 0.018 (true: 0.000)\n",
      "        [final] Log-likelihood: -673.72 (per sample: -0.3369)\n",
      "        Log-likelihood using true params: -1060.33 (per sample: -0.5302)\n",
      "    Subject 4 / 4\n",
      "        Parameters estimates (subject-level fit)\n",
      "            [subject] type1_noise: 0.508 ± 0.049 (true: 0.500)\n",
      "            [subject] type1_nonlinear_gain: 0.386 ± 0.339 (true: 0.500)\n",
      "            [subject] type1_bias: 0.005 ± 0.019 (true: 0.000)\n",
      "        [subject] Log-likelihood: -733.73 (per sample: -0.3669)\n",
      "        [subject] Fitting time: 0.12 secs\n",
      "        Parameters estimates (group-level fit)\n",
      "            [subject] type1_noise: 0.507 ± 0.022 (true: 0.500)\n",
      "            [group=random] type1_nonlinear_gain: 0.382 ± 0.090 (true: 0.500)\n",
      "            [subject] type1_bias: 0.005 ± 0.019 (true: 0.000)\n",
      "        [final] Log-likelihood: -733.73 (per sample: -0.3669)\n",
      "        Log-likelihood using true params: -1119.98 (per sample: -0.56)\n",
      "Type 1 level finished\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the current example, all participant estimates for `type1_nonlinear_gain` are pretty similar, reflecting the fact that the data were in fact generated by the same ground truth model:",
   "id": "8fad9123e124147"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T20:04:00.328957515Z",
     "start_time": "2026-02-17T20:04:00.266703902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for s in range(result.nsubjects):\n",
    "    print(f'Subject {s}')\n",
    "    for k, v in result.type1.params[s].items():\n",
    "        print(f'\\t{k}: {v:.6f}')"
   ],
   "id": "4db6624a175a8d7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0\n",
      "\ttype1_noise: 0.519667\n",
      "\ttype1_nonlinear_gain: 0.389388\n",
      "\ttype1_bias: -0.022693\n",
      "Subject 1\n",
      "\ttype1_noise: 0.506679\n",
      "\ttype1_nonlinear_gain: 0.360708\n",
      "\ttype1_bias: -0.027539\n",
      "Subject 2\n",
      "\ttype1_noise: 0.461593\n",
      "\ttype1_nonlinear_gain: 0.393358\n",
      "\ttype1_bias: 0.007012\n",
      "Subject 3\n",
      "\ttype1_noise: 0.506998\n",
      "\ttype1_nonlinear_gain: 0.381504\n",
      "\ttype1_bias: 0.005352\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This observation is matched by an inspection of the population estimate for `type1_nonlinear_gain`:",
   "id": "8a8f1246dfe672f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T20:04:00.395728666Z",
     "start_time": "2026-02-17T20:04:00.329890969Z"
    }
   },
   "cell_type": "code",
   "source": "print(f'Estimated population mean: {result.params_random_effect.mean['type1_nonlinear_gain']:.3f}')",
   "id": "fe0b0646ccc1832d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated population mean: 0.384\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Priors\n",
    "\n",
    "Priors present another way to inform and regularize point estimates of participants. If there is good reason from prior literature or a prior study to assume a prior distribution for a parameter, one can perform Maximum A Posteriori estimation (MAP) instead of Maximum Likelihood estimation (MLE). In Remeta this is possible by specifying the `prior` attribute of a parameter. In the following example, we delete the previous random effect for `type1_param_nonlinear_gain` and specify a prior instead - a tuple of the form (prior_mean, prior_std)."
   ],
   "id": "c3a23f9c85bd3468"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T20:04:01.529368095Z",
     "start_time": "2026-02-17T20:04:00.397537326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cfg.param_type1_nonlinear_gain.group = None\n",
    "cfg.param_type1_nonlinear_gain.prior = (0, 0.25)\n",
    "rem = remeta.ReMeta(cfg=cfg)\n",
    "rem.fit(data.stimuli, data.choices, data.confidence)\n",
    "result = rem.summary()"
   ],
   "id": "12c7cec09d8d2eb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset characteristics:\n",
      "    No. subjects: 4\n",
      "    No. samples: [2000, 2000, 2000, 2000]\n",
      "    Accuracy: 84.4% correct\n",
      "    d': 2.023\n",
      "    Choice bias: -0.4%\n",
      "\n",
      "+++ Type 1 level +++\n",
      "  Subject-level estimation (MLE)\n",
      "     Subject 1 / 4\n",
      "     Subject 2 / 4\n",
      "     Subject 3 / 4\n",
      "     Subject 4 / 4\n",
      "    .. finished (1.1 secs).\n",
      "  Final report\n",
      "    Subject 1 / 4\n",
      "        Parameters estimates (subject-level fit)\n",
      "            [subject] type1_noise: 0.492 ± 0.032 (true: 0.500)\n",
      "            [subject+prior=N(0,0.25)] type1_nonlinear_gain: 0.189 ± 0.188 (true: 0.500)\n",
      "            [subject] type1_bias: -0.021 ± 0.018 (true: 0.000)\n",
      "        [subject] Log-likelihood: -749.28 (per sample: -0.3746)\n",
      "        [subject] Fitting time: 0.20 secs\n",
      "        Log-likelihood using true params: -1177.70 (per sample: -0.5888)\n",
      "    Subject 2 / 4\n",
      "        Parameters estimates (subject-level fit)\n",
      "            [subject] type1_noise: 0.463 ± 0.029 (true: 0.500)\n",
      "            [subject+prior=N(0,0.25)] type1_nonlinear_gain: 0.047 ± 0.177 (true: 0.500)\n",
      "            [subject] type1_bias: -0.025 ± 0.017 (true: 0.000)\n",
      "        [subject] Log-likelihood: -735.27 (per sample: -0.3676)\n",
      "        [subject] Fitting time: 0.17 secs\n",
      "        Log-likelihood using true params: -1169.71 (per sample: -0.5849)\n",
      "    Subject 3 / 4\n",
      "        Parameters estimates (subject-level fit)\n",
      "            [subject] type1_noise: 0.440 ± 0.025 (true: 0.500)\n",
      "            [subject+prior=N(0,0.25)] type1_nonlinear_gain: 0.208 ± 0.192 (true: 0.500)\n",
      "            [subject] type1_bias: 0.007 ± 0.017 (true: 0.000)\n",
      "        [subject] Log-likelihood: -674.50 (per sample: -0.3372)\n",
      "        [subject] Fitting time: 0.24 secs\n",
      "        Log-likelihood using true params: -1060.33 (per sample: -0.5302)\n",
      "    Subject 4 / 4\n",
      "        Parameters estimates (subject-level fit)\n",
      "            [subject] type1_noise: 0.476 ± 0.030 (true: 0.500)\n",
      "            [subject+prior=N(0,0.25)] type1_nonlinear_gain: 0.154 ± 0.186 (true: 0.500)\n",
      "            [subject] type1_bias: 0.005 ± 0.018 (true: 0.000)\n",
      "        [subject] Log-likelihood: -734.19 (per sample: -0.3671)\n",
      "        [subject] Fitting time: 0.23 secs\n",
      "        Log-likelihood using true params: -1119.98 (per sample: -0.56)\n",
      "Type 1 level finished\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "According to our prior, a null effect for the nonlinearity parameter should be most likely. The precision of the prior is moderate with a standard deviation of `1`. As a consequence, the nonlinearity parameter is biased towards 0 compared to the original estimates without a prior:",
   "id": "bf71528973e45a9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-17T20:04:01.655750907Z",
     "start_time": "2026-02-17T20:04:01.585562627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for s in range(result.nsubjects):\n",
    "    print(f'Subject {s}')\n",
    "    for k, v in result.type1.params[s].items():\n",
    "        print(f'\\t{k}: {v:.3f}')"
   ],
   "id": "f4167fe7d8b3de60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0\n",
      "\ttype1_noise: 0.492\n",
      "\ttype1_nonlinear_gain: 0.189\n",
      "\ttype1_bias: -0.021\n",
      "Subject 1\n",
      "\ttype1_noise: 0.463\n",
      "\ttype1_nonlinear_gain: 0.047\n",
      "\ttype1_bias: -0.025\n",
      "Subject 2\n",
      "\ttype1_noise: 0.440\n",
      "\ttype1_nonlinear_gain: 0.208\n",
      "\ttype1_bias: 0.007\n",
      "Subject 3\n",
      "\ttype1_noise: 0.476\n",
      "\ttype1_nonlinear_gain: 0.154\n",
      "\ttype1_bias: 0.005\n"
     ]
    }
   ],
   "execution_count": 35
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
