# Validation Schedules
The tasks discussed in this section has to be implemented in `/ValidationSchedules`.

## Parameter Recovery 

The tasks discussed in this section has to be implemented in `/ValidationSchedules/ParameterRecovery`.

### Agenda
 Verify if ReMeta fitted to the synthetic data generated by ReMeta with a specific parameter set, can *recover* or re-estimate the same parameter set. Even if not exactly, to what degree.

The primary goal is to package the entire analysis done in `/demo/basic_usage.ipynb` into one coherent pipeline(class) - call it `ParameterRecovery()` class. Next, use this `ParameterRecovery()' to build an interfacing script - this script, called `param_recovery_interface.py` would give the user a front-end to:

1. Specify multiple parameter sets. Each set will have an ID.
2. Run each `ParameterRecovery()` pipeline in parallel - maybe using Python's multiprocessing capabilities.
3. Collect the results of each run.
4. Store each result with the proper parameter set ID.

### Basic workflow for `ParameterRecovery()`
This is completely based on `/demo/basic_usage.ipynb`. Reuse the code from `/demo/basic_usage.ipynb` to create this class. The workflow is as follows:

1. Specify a parameter set: $P$
2. Use $ReMeta(P)$ to generate a synthetic dataset with proper specification of dataset size(number of trials and participants): $D$
3. Fit ReMeta on $D$ to get a new parameter set: $\hat P = ReMeta(P,D)$
4. Estimate similarity between $P$ and $\hat P$: $\Delta = (P \sim \hat P)$.\
 For now, let the measure of similarity be simply the difference between the estimated parameter and the true parameter for integer and floating point parameters - a positive value would mean the overestimation and vice versa - and *True/False* for boolean parameters. We could also use **AIC** values of *ReMeta* with fitted paramters and with true parameters - smaller/equal AIC on parameters inferred by *ReMeta* against the true parameters would mean success for *ReMeta*.

5. Save as a dictionary `run_dict.pkl`: $P$, $D$, $\hat P$ and $\Delta$

**NOTE:** Multiple steps executed in `/demo/basic_usage.ipynb` generate terminal outputs and plots. A file called `<ID>_out.txt` must be created where the step by step terminal outputs are written where `ID` is the run ID the class gets from the `param_recovery_interface.py` script. All the plots generated and the output file are to be saved in a folder named `run_<ID>`. The `run_dict.pkl` dictionary containing $P$, $D$, $\hat P$ and $\Delta$ is also saved in `/run_<ID>`.

### Interface
The interace in `param_recovery_interface.py` does the following:

0. A function called `parallel_execution(n, jobs)` is created where `n` specifies the number of workers or cores and `jobs` specifies the list of 'ParameterRecovery()` objects to execute. `n` defaults to the total number of CPU cores available. This function also shows a progress bar.

1. Defines a variable called `experiment_id`. This is a string which specifies the ID of the current experiment. A subdirectory of name `experiment_id` is created inside `/Experimentations/ParameterRecovery`. 

2. A variable called `primary_storage_dir` is created with the path to the `experiment_id` folder created in Step 1.

3. A dictionary of parameter dictionaries is defined: `Psets = {<ID>: dict(parameters as specified in basic_usage.ipynb)}`. Here the keys are the IDs of the different runs and the values are the corresponding parameter dictionaries that the user will specify.

4. Multiple `ParameterRecovery()` objects are created, each one getting as input to the constructor: `primary_storage_dir`, a key from Psets (the run ID) and the corresponding Psets value(the parameter set for the corresponding run ID). All of these objects are stored in a list called `parameter_recovery_runs_list`.

5. `parameter_recovery_runs_list` is fed to the `parallel_execution(n, jobs)` function.


### Analysis Dashboard
After successful execution of `param_recovery_interface.py`, we need to analyze the saved results. This will be done in a jupyter notebook called `ParameterRecoveryResultsDashboard.ipynb` and also in a simple trendy-looking webapp of the same name.

#### The notebook
1. Start with the variable `primary_storage_dir` and `experiment_id`. The user will specify these values.

2. List runs found in the directory.

3. Define a utility function called `load_param_recov_results()` that loads the saved results in `run_dict.pkl` and create a dataframe called `results_df` from this dictionary - rows are the parameters and the columns are `True`, `Estimated` and `Delta`. Return this dataframe.

4. Use `load_param_recov_results()` for each run and store each `results_df`.

5. Define another utility function called `load_param_recov_figs()`that given the run ID loads, returns and shows all the figures generated in that run.

6. Use `load_param_recov_figs()` for a single run ID.

7. Define another utility function called `load_param_recov_history()` that given the run ID loads and shows all the fitting history from the `<ID>_out.txt` file.

8. Use `load_param_recov_history()` for a single run ID.

#### The webapp
1. This has all the functionalities of the notebook. 

2. The webapp is segmented into 2 primary sections.\
    2.1 A slim side panel on the top left corner that takes in the `primary_storage_dir` as input. Given this input, it shows a list of check boxes with the run IDs next to it. Multiple run IDs can be selected. \
    2.2 For each run ID the broad right panel is segmented horizontally into a tab - one big horizontal tab for one run ID.

3. When a run ID check box is selected, the corresponding big horizontal tab is segmented into 3 subtabs. \
    3.1 The first subtab shows the `results_df` as a table. \
    3.2 The second subtab shows another checkbox list but this time for the figures. This tab will show all the figures selected in the figures checkbox list. This might take some space, so we can't hardcode the size of the tabs, they'd have to be adaptable or scrollable. \
    3.3 The third subtab will show the fitting history. This could be long, so it should be scrollable or the size should be adaptable.

4. The entire right panel shows results for multiple runs in horizontal tabs. If there are 10 runs, of course 10 such big horizontal tabs won't fitin one static page. So, this right panel also has to be scrollable.




# Possible Redactions or $\alpha$ Decorations
1. Variety of `meta_link_function`s in `configuration.py`. \
Final candidates: `probability_correct`,  `{x}_criteria`. \
Prospective candidate: `neural regression`